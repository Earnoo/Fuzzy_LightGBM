{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv(r'\\M5 datasets\\sales_train_evaluation.csv')\n",
    "calendar = pd.read_csv(r'\\M5 datasets\\calendar.csv')\n",
    "calendar = calendar.loc[calendar['year']!=2011]\n",
    "calendar = calendar.loc[calendar['year']!=2016]\n",
    "\n",
    "df.iloc[:,6:] = df.iloc[:,6:].replace(0, np.nan)\n",
    "df['nan'] = df.iloc[:,6:].T.isna().sum().T\n",
    "\n",
    "## Filter data for Foods Category\n",
    "\n",
    "t = df.loc[df['cat_id']=='FOODS']\n",
    "days_lst = calendar['d'].unique().tolist()\n",
    "spd = t[['id','item_id','dept_id','store_id']]\n",
    "spd [days_lst] = t[days_lst]\n",
    "\n",
    "# calendar.loc[calendar['month']==1]\n",
    "calendar.loc[calendar['month']==1, 'q'] = 1\n",
    "calendar.loc[calendar['month']==2, 'q'] = 1\n",
    "calendar.loc[calendar['month']==3, 'q'] = 1\n",
    "\n",
    "\n",
    "calendar.loc[calendar['month']==4, 'q'] = 2\n",
    "calendar.loc[calendar['month']==5, 'q'] = 2\n",
    "calendar.loc[calendar['month']==6, 'q'] = 2\n",
    "\n",
    "calendar.loc[calendar['month']==7, 'q'] = 3\n",
    "calendar.loc[calendar['month']==8, 'q'] = 3\n",
    "calendar.loc[calendar['month']==9, 'q'] = 3\n",
    "\n",
    "calendar.loc[calendar['month']==10, 'q'] = 4\n",
    "calendar.loc[calendar['month']==11, 'q'] = 4\n",
    "calendar.loc[calendar['month']==12, 'q'] = 4\n",
    "\n",
    "\n",
    "spd_main = pd.DataFrame()\n",
    "i=0\n",
    "for y in [2012,2013,2014,2015]:\n",
    "    for q in [1,2,3,4]:\n",
    "        df_main = pd.DataFrame()\n",
    "        df_main = spd[['id','item_id','dept_id','store_id']]\n",
    "        lst = calendar.loc[(calendar['q']==1)&(calendar['year']==y)]['d'].unique().tolist()\n",
    "        for i in range(0,len(lst)):\n",
    "            df_main[str(i)] = spd[lst[i]]\n",
    "        df_main['q'] = q\n",
    "        df_main['y'] = y\n",
    "        i=i+1\n",
    "        if i ==1:\n",
    "            spd_main = df_main\n",
    "        else:\n",
    "            spd_main = pd.concat([spd_main, df_main], axis = 0)\n",
    "        \n",
    " \n",
    "# Filter 44% Nan\n",
    "spd_main['nan'] = spd_main.iloc[:, 4:-2].T.isna().sum().T\n",
    "spd_main = spd_main.loc[spd_main['nan']<=40]\n",
    "del spd_main['nan']\n",
    "\n",
    "# TS\n",
    "TS = spd_main.copy()\n",
    "TS.to_csv(r'TS.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuzzification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spd_main.iloc[:, 4:-2] = spd_main.iloc[:, 4:-2].fillna(0)\n",
    "\n",
    "# Define alpha1 function\n",
    "def alpha1(alphacu, kcu):\n",
    "    alphamoi = alphacu / (1 + alphacu * kcu)\n",
    "    return alphamoi.T\n",
    "\n",
    "\n",
    "class FuzzificationProcessor:\n",
    "    def __init__(self):\n",
    "        self.i = 0\n",
    "        self.data = []\n",
    "        self.rmse = []\n",
    "    \n",
    "    def fuzzification(self, row_array):\n",
    "       \n",
    "        W = row_array\n",
    "        zt = (W - np.min(W)) / (np.max(W) - np.min(W))\n",
    "        l = 12\n",
    "\n",
    "        # Initial distance and mean distance calculation\n",
    "        d = squareform(pdist(zt.reshape(-1, 1), 'euclidean'))\n",
    "        d1 = np.triu(d, 1)\n",
    "        cn = (zt.size * (zt.size - 1)) / 2\n",
    "        mu = np.sum(d1) / cn\n",
    "\n",
    "        # Standard deviation calculation\n",
    "        tong = (d - mu) ** 2\n",
    "        tt = np.triu(tong, 1)\n",
    "        sigma = np.sqrt(np.sum(tt) / cn)\n",
    "\n",
    "        # Weight matrix calculation\n",
    "        f = np.exp(-d / (sigma / l)) * (d <= mu)\n",
    "        np.fill_diagonal(f, 0)\n",
    "\n",
    "        # Iterative update of ztmoi\n",
    "        iter_count = 0\n",
    "        exilanh = 0.01\n",
    "\n",
    "        j = 0\n",
    "        while True:\n",
    "            tu = np.dot(f, zt)\n",
    "            mau = np.sum(f, axis=1)\n",
    "            mau = np.where(mau == 0, 0.00000000001, mau)\n",
    "            ztmoi = tu / mau\n",
    "            \n",
    "            if np.max(np.abs(ztmoi - zt)) <= exilanh:\n",
    "                break\n",
    "            zt = ztmoi\n",
    "            iter_count += 1\n",
    "            d = squareform(pdist(zt.reshape(-1, 1), 'euclidean'))\n",
    "            d1 = np.triu(d, 1)\n",
    "            mu = np.sum(d1) / cn\n",
    "            tong = (d - mu) ** 2\n",
    "            tt = np.triu(tong, 1)\n",
    "            sigma = np.sqrt(np.sum(tt) / cn)\n",
    "            alp = alpha1(np.ones((zt.size, zt.size)), f)\n",
    "            f = np.exp(-d / (sigma / l)) * (d <= mu)\n",
    "            np.fill_diagonal(f, 0)\n",
    "            j=j+1\n",
    "            if j>=2000:   # For faster runtime\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate U_new\n",
    "        # epsilon = 1e-2\n",
    "        \n",
    "        \n",
    "        # center = np.unique(np.round(ztmoi, 2))\n",
    "        \n",
    "        # dist = squareform(pdist(center.reshape(-1, 1)))\n",
    "        # dist[dist == 0] = epsilon\n",
    "        # tmp = dist ** (-2 / (2 - 1))\n",
    "        # U_new = tmp / np.sum(tmp, axis=0)\n",
    "\n",
    "\n",
    "   \n",
    "        # self.data.append({\n",
    "        #     'indx' : self.i,\n",
    "        #     'min' : np.min(W),\n",
    "        #     'max' : np.max(W),\n",
    "        #     'Umatrix' : U_new,\n",
    "        #     }\n",
    "        #     )\n",
    "        # self.i += 1\n",
    "        # print(np.round(self.i/740, 2))\n",
    "        FTS = zt* (np.max(W) - np.min(W)) + np.min(W)\n",
    "        FTS = np.round(FTS, 1)\n",
    "\n",
    "        if np.any(np.isnan(FTS)):\n",
    "            FTS = W.copy()\n",
    "        self.rmse.append(np.sqrt((abs(W-FTS))**2).mean())\n",
    "        return FTS\n",
    "\n",
    "processor = FuzzificationProcessor()\n",
    "\n",
    "spd_main.iloc[:, 4:-2] = spd_main.iloc[:, 4:-2].apply(lambda row: pd.Series(processor.fuzzification(row.to_numpy())), axis=1)\n",
    "\n",
    "# processed_data = processor.data\n",
    "\n",
    "spd_main.to_csv(r'FTS.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df, threshold):\n",
    "        numeric_df = df.select_dtypes(include=['number'])\n",
    "        \n",
    "        col_corr = set()\n",
    "        corr_matrix = numeric_df.corr()\n",
    "        \n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                    colname = corr_matrix.columns[i]\n",
    "                    col_corr.add(colname)\n",
    "        \n",
    "        return col_corr\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import  train_test_split\n",
    "def train(trget, real_df, label, year):\n",
    "    \n",
    "    \n",
    "    df = trget.copy()\n",
    "    df['id'] = df['id'].astype('category') \n",
    "    df['item_id'] = df['item_id'].astype('category') \n",
    "    df['dept_id'] = df['dept_id'].astype('category') \n",
    "    df['store_id'] = df['store_id'].astype('category') \n",
    "    X = df.copy()\n",
    "    \n",
    "    ################\n",
    "    real_df_y = real_df.loc[real_df['y']==2015][str(label)]\n",
    "\n",
    "    eval_y = real_df_y\n",
    "\n",
    "    #########################\n",
    "    y = X.loc[X['y']<year][str(label)]\n",
    "    # eval_y = eval_X[str(label)]\n",
    "\n",
    "    for l in range(label, 91):\n",
    "          del X[str(l)]\n",
    "          \n",
    "\n",
    "    #####################################################Feature Extraction\n",
    "    last_y = label - 1\n",
    "\n",
    "    icols =  [['id'],\n",
    "                ['item_id'],\n",
    "                ['dept_id'],\n",
    "                ['store_id']]\n",
    "    for d in [last_y-1, last_y-2, last_y-3, last_y-4, last_y-5]:\n",
    "            for col in icols:\n",
    "                    col_name = '_' + '_'.join(col) + '_'\n",
    "                    X['enc' + col_name +str(last_y - d)+ '_mean'] = X.groupby(col)[str(d)].transform('mean')\n",
    "                    X['enc' + col_name + str(last_y - d)+ '_median'] = X.groupby(col)[str(d)].transform('median')\n",
    "                    X['enc' + col_name +str(last_y - d)+ '_std']    = X.groupby(col)[str(d)].transform('std')\n",
    "\n",
    "                    X['enc' + col_name + str(last_y - d)+ '_mean'] = X.groupby(col)['enc' + col_name +str(last_y - d)+ '_mean'].transform(lambda x: x.fillna(method='bfill').fillna(method='ffill'))\n",
    "                    X['enc' + col_name + str(last_y - d)+ '_median'] = X.groupby(col)['enc' + col_name + str(last_y - d)+ '_median'].transform(lambda x: x.fillna(method='bfill').fillna(method='ffill'))\n",
    "                    X['enc' + col_name +str(last_y - d)+ '_std'] = X.groupby(col)['enc' + col_name + str(last_y - d)+ '_std'] .transform(lambda x: x.fillna(method='bfill').fillna(method='ffill'))\n",
    "\n",
    "    for i in [2, 3, 7, 14, 30]:\n",
    "        r = pd.DataFrame()\n",
    "        for j in range(0,i):\n",
    "            r[str(j)] = X[str(last_y-j)]\n",
    "            \n",
    "        X['rolling_mean_' + str(i)]   = r.mean(axis=1)\n",
    "        X['rolling_median_' + str(i)] = r.median(axis=1)\n",
    "        X['rolling_std_' + str(i)]    = r.std(axis=1)\n",
    "        X['rolling_max_' + str(i)]    = r.max(axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "    cor_feature = correlation(X, 0.97)\n",
    "    features_columns = X.columns.tolist()\n",
    "    features_columns = [x for x in features_columns if x not in cor_feature]\n",
    "\n",
    "    eval_X = X.loc[X['y']==2015]\n",
    "    X = X.loc[X['y']<year]\n",
    "    X = X.drop(columns = cor_feature)\n",
    "    \n",
    "    eval_X = eval_X.drop(columns = cor_feature)\n",
    "    print('delete this cols for corr:', cor_feature)\n",
    "    \n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(eval_X.shape)\n",
    "    print(eval_y.shape)\n",
    "    #######################################################################\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30,shuffle=True)\n",
    "\n",
    "\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',          \n",
    "        'tweedie_variance_power': 1.2,      \n",
    "        'metric': 'rmse',                   \n",
    "        'subsample': 0.5,                 \n",
    "        'subsample_freq': 1,             \n",
    "        'learning_rate': 0.005,          \n",
    "    #     'max_depth' : 12,                 \n",
    "    #     'early_stopping_round' : 1,         \n",
    "        'num_leaves': 2**12-1,\n",
    "        'min_data_in_leaf': 2**8-1,           \n",
    "        'feature_fraction': 0.5,           \n",
    "        'max_bin': 100,                     \n",
    "        'n_estimators': 15000,               \n",
    "        'boost_from_average': False,        \n",
    "        'verbose': -1,                      \n",
    "        'SEED': 42\n",
    "    }\n",
    "    le_callback = lgb.log_evaluation(5)\n",
    "    es_callback = lgb.early_stopping(5)\n",
    "\n",
    "    train_data = lgb.Dataset(X_train,\n",
    "                            label = y_train)\n",
    "    valid_data = lgb.Dataset(X_test,\n",
    "                            label = y_test)\n",
    "    \n",
    "    evals_result = {}\n",
    "\n",
    "    estimator = lgb.train(\n",
    "            params,\n",
    "            train_set=train_data,\n",
    "            valid_sets=[train_data, valid_data],\n",
    "            valid_names=['train', 'valid'],\n",
    "            callbacks=[lgb.record_evaluation(evals_result),le_callback, es_callback],\n",
    "            \n",
    "        )\n",
    "    epochs = len(evals_result['train']['rmse'])\n",
    "    x_axis = range(0, epochs)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x_axis, evals_result['train']['rmse'], label='Train Loss')\n",
    "    plt.plot(x_axis, evals_result['valid']['rmse'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE Loss')\n",
    "    plt.title('LightGBM RMSE Loss'+', cv:'+str(year - 1))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    FEA = pd.DataFrame({'name':estimator.feature_name(),\n",
    "                            'importance_gain': estimator.feature_importance(importance_type='gain'),\n",
    "                            #'importance_split': estimator.feature_importance(importance_type='split'),\n",
    "                            }).sort_values('importance_gain', ascending=False)\n",
    "    display(FEA.head(30))\n",
    "\n",
    "\n",
    "    y_hat = estimator.predict(eval_X)\n",
    "    eval_df = pd.DataFrame()\n",
    "    eval_y=eval_y.reset_index(drop=True)\n",
    "    eval_X=eval_X.reset_index(drop=True)\n",
    "    y_hat = estimator.predict(eval_X)\n",
    "    eval_df['pred']=y_hat\n",
    "    eval_df['real'] = eval_y\n",
    "    eval_df['MAPE'] = abs(eval_df['pred'] - eval_df['real'])/eval_df['real']\n",
    "    eval_df['squar_error'] = (abs(eval_df['pred'] - eval_df['real']))**2\n",
    "\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model for TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 2\n",
    "for c in range(1, cv + 1):\n",
    "    year = 2015 - c\n",
    "    print('cv '+str(c)+':')\n",
    "    ts_result = train(TS, TS, 89, year)\n",
    "    print(ts_result)\n",
    "    print('RMSE without zeros in real : ', ts_result.loc[ts_result['real']!=0]['squar_error'].mean())\n",
    "    print(\"MAPE without zeros in real : \",ts_result.loc[ts_result['real']!=0]['APE'].mean())\n",
    "    print('RMSE : ', ts_result['squar_error'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 2\n",
    "for c in range(1, cv + 1):\n",
    "    year = 2015 - c\n",
    "    print('cv '+str(c)+':')\n",
    "    fts_result = train(FTS, TS, 89, year)\n",
    "    print(fts_result)\n",
    "    print('RMSE without zeros in real : ', fts_result.loc[fts_result['real']!=0]['squar_error'].mean())\n",
    "    print(\"MAPE without zeros in real : \",fts_result.loc[fts_result['real']!=0]['APE'].mean())\n",
    "    print('RMSE : ', fts_result['squar_error'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
